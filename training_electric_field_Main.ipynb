{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e417dbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from torch import nn\n",
    "from time import time\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import utils\n",
    "from unet_model import ConvAE, ConvAE_M, ConvAE_M_Loc, ConvAE_M_shallow, UNet, UNet_M, UNet_M_Loc, UNet_M_Loc_strided, UNet_M_Loc_deepNet, UNet_M_LC, WNet, WNet2\n",
    "import torchvision\n",
    "import random \n",
    "\n",
    "from models import Linear, Linear_M, Linear_M_Loc, LinearAE, LinearAE_M, LinearAE_M_Loc\n",
    "\n",
    "font = {'size'   : 16}\n",
    "\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "freq = ['400MHz','900MHz','1pt5GHz','2pt4GHz'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75b3ff7-17fd-4650-8471-f602f70d2b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs : train\n",
    "f_f1        = os.path.join('data', 'E_field_COMSOL', freq , 'data_f1_04_' + freq + '.pt')\n",
    "f_f1_05     = os.path.join('data', 'E_field_COMSOL', freq , 'data_f1_05_' + freq + '.pt')\n",
    "f_f1_06     = os.path.join('data', 'E_field_COMSOL', freq , 'data_f1_06_' + freq + '.pt')\n",
    "f_f1_18     = os.path.join('data', 'E_field_COMSOL', freq , 'data_f1_18_' + freq + '.pt')\n",
    "f_f1_20     = os.path.join('data', 'E_field_COMSOL', freq , 'data_f1_20_' + freq + '.pt')\n",
    "f_f1_38     = os.path.join('data', 'E_field_COMSOL', freq , 'data_f1_38_' + freq + '.pt')\n",
    "f_f1_41     = os.path.join('data', 'E_field_COMSOL', freq , 'data_f1_41_' + freq + '.pt')\n",
    "f_f1_42     = os.path.join('data', 'E_field_COMSOL', freq , 'data_f1_42_' + freq + '.pt')\n",
    "f_f1_43     = os.path.join('data', 'E_field_COMSOL', freq , 'data_f1_43_' + freq + '.pt')\n",
    "f_f1_44     = os.path.join('data', 'E_field_COMSOL', freq , 'data_f1_44_' + freq + '.pt')\n",
    "f_f1_45     = os.path.join('data', 'E_field_COMSOL', freq , 'data_f1_45_' + freq + '.pt')\n",
    "f_f1_46     = os.path.join('data', 'E_field_COMSOL', freq , 'data_f1_46_' + freq + '.pt')\n",
    "f_f1_47     = os.path.join('data', 'E_field_COMSOL', freq , 'data_f1_47_' + freq + '.pt')\n",
    "f_f1_48     = os.path.join('data', 'E_field_COMSOL', freq , 'data_f1_48_' + freq + '.pt')\n",
    "f_f1_49     = os.path.join('data', 'E_field_COMSOL', freq , 'data_f1_49_' + freq + '.pt')\n",
    "# inputs: val\n",
    "f_f1_50     = os.path.join('data', 'E_field_COMSOL', freq , 'data_f1_50_' + freq + '.pt')\n",
    "f_f1_51     = os.path.join('data', 'E_field_COMSOL', freq , 'data_f1_51_' + freq + '.pt')\n",
    "f_f1_52     = os.path.join('data', 'E_field_COMSOL', freq , 'data_f1_52_' + freq + '.pt')\n",
    "f_f1_53     = os.path.join('data', 'E_field_COMSOL', freq , 'data_f1_53_' + freq + '.pt')\n",
    "f_f1_54     = os.path.join('data', 'E_field_COMSOL', freq , 'data_f1_54_' + freq + '.pt')\n",
    "\n",
    "# loc: train + test\n",
    "f_loc    = os.path.join('data', 'E_field_COMSOL', freq, 'f_loc_outside.pt')\n",
    "\n",
    "# MRI\n",
    "f_MRI    = os.path.join('data', 'E_field_COMSOL', freq, 'MRI_04.pt')\n",
    "f_MRI_05    = os.path.join('data', 'E_field_COMSOL', freq, 'MRI_05.pt')\n",
    "f_MRI_06    = os.path.join('data', 'E_field_COMSOL', freq, 'MRI_06.pt')\n",
    "f_MRI_18    = os.path.join('data', 'E_field_COMSOL', freq, 'MRI_18.pt')\n",
    "f_MRI_20    = os.path.join('data', 'E_field_COMSOL', freq, 'MRI_20.pt')\n",
    "f_MRI_38    = os.path.join('data', 'E_field_COMSOL', freq, 'MRI_38.pt')\n",
    "f_MRI_41    = os.path.join('data', 'E_field_COMSOL', freq, 'MRI_41.pt')\n",
    "f_MRI_42    = os.path.join('data', 'E_field_COMSOL', freq, 'MRI_42.pt')\n",
    "f_MRI_43    = os.path.join('data', 'E_field_COMSOL', freq, 'MRI_43.pt')\n",
    "f_MRI_44    = os.path.join('data', 'E_field_COMSOL', freq, 'MRI_44.pt')\n",
    "f_MRI_45    = os.path.join('data', 'E_field_COMSOL', freq, 'MRI_45.pt')\n",
    "f_MRI_46    = os.path.join('data', 'E_field_COMSOL', freq, 'MRI_46.pt')\n",
    "f_MRI_47    = os.path.join('data', 'E_field_COMSOL', freq, 'MRI_47.pt')\n",
    "f_MRI_48    = os.path.join('data', 'E_field_COMSOL', freq, 'MRI_48.pt')\n",
    "f_MRI_49    = os.path.join('data', 'E_field_COMSOL', freq, 'MRI_49.pt')\n",
    "f_MRI_50    = os.path.join('data', 'E_field_COMSOL', freq, 'MRI_50.pt')\n",
    "f_MRI_51    = os.path.join('data', 'E_field_COMSOL', freq, 'MRI_51.pt')\n",
    "f_MRI_52    = os.path.join('data', 'E_field_COMSOL', freq, 'MRI_52.pt')\n",
    "f_MRI_53    = os.path.join('data', 'E_field_COMSOL', freq, 'MRI_53.pt')\n",
    "f_MRI_54    = os.path.join('data', 'E_field_COMSOL', freq, 'MRI_54.pt')\n",
    "\n",
    "\n",
    "n_batch = 32\n",
    "n_workers = 0\n",
    "n_subsample = 3 \n",
    "\n",
    "total_train_patients = 14\n",
    "impulse_mag = 1\n",
    "MRI_loc_impulse_msg = 10 # for setting the size of the antenna impulse \n",
    "\n",
    "# choosing seed randomly from(0, 500)\n",
    "seed_n = 500*torch.rand(1)\n",
    "seed = int(seed_n)\n",
    "\n",
    "\n",
    "orig_size = [434,362]\n",
    "orig_size_down = [int(a/n_subsample) for a in orig_size] \n",
    "\n",
    "data_train = 'f1' #list picks which frequency to use\n",
    "#device = torch.device('cuda')\n",
    "device = torch.device('cpu')\n",
    "\n",
    "model_type = ['Linear', 'Linear_M', 'Linear_M_Loc', \n",
    "              'LinearAE', 'LinearAE_M', 'LinearAE_M_Loc',\n",
    "              'ConvAE', 'ConvAE_M', 'ConvAE_M_Loc',\n",
    "              'unet', 'unet_M', 'UNet_M_LC', 'UNet_M_Loc', 'UNet_M_Loc_strided', 'UNet_M_Loc_deepNet',\n",
    "              'wnet', 'wnet2'][13] #list picks which model to use\n",
    "print(model_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157dedb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.set_seed(seed)\n",
    "\n",
    "\n",
    "f_log = f'Hyper_param_'+str(model_type)+ '_'+ freq + '_D' + str(n_subsample) + '.txt'\n",
    "f_subfolder = model_type + '_D' + str(n_subsample) + '_' + data_train \n",
    "\n",
    "\n",
    "f_output_images = os.path.join('output', freq,  'images', model_type)\n",
    "f_model_output = os.path.join('output', freq, 'models')\n",
    "\n",
    "# make output folders\n",
    "os.makedirs(os.path.join(f_output_images, f_subfolder), exist_ok=True)\n",
    "os.makedirs(os.path.join(f_model_output, f_subfolder), exist_ok=True)\n",
    "\n",
    "f_sub_sub_folder = f'seed: {seed}'\n",
    "os.makedirs(os.path.join(f_output_images, f_subfolder, f_sub_sub_folder), exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c79edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complex_resize(x, shape):\n",
    "    x_real = x.real.clone()\n",
    "    x_imag = x.imag.clone()\n",
    "    x_real_resize = torchvision.transforms.Resize(shape, antialias=True)(x_real)\n",
    "    x_imag_resize = torchvision.transforms.Resize(shape, antialias=True)(x_imag)    \n",
    "    y = x_real_resize + 1j* x_imag_resize\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42eacd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load  train data + val data\n",
    "fdata = torch.load(f_f1).to(device)\n",
    "fdata_05 = torch.load(f_f1_05).to(device)\n",
    "fdata_06 = torch.load(f_f1_06).to(device)\n",
    "fdata_18 = torch.load(f_f1_18).to(device)\n",
    "fdata_20 = torch.load(f_f1_20).to(device)\n",
    "fdata_38 = torch.load(f_f1_38).to(device)\n",
    "fdata_41 = torch.load(f_f1_41).to(device)\n",
    "fdata_42 = torch.load(f_f1_42).to(device)\n",
    "fdata_43 = torch.load(f_f1_43).to(device)\n",
    "fdata_44 = torch.load(f_f1_44).to(device)\n",
    "fdata_45 = torch.load(f_f1_45).to(device)\n",
    "fdata_46 = torch.load(f_f1_46).to(device)\n",
    "fdata_47 = torch.load(f_f1_47).to(device)\n",
    "fdata_48 = torch.load(f_f1_48).to(device)\n",
    "fdata_49 = torch.load(f_f1_49).to(device)\n",
    "fdata_50 = torch.load(f_f1_50).to(device)\n",
    "fdata_51 = torch.load(f_f1_51).to(device)\n",
    "fdata_52 = torch.load(f_f1_52).to(device)\n",
    "fdata_53 = torch.load(f_f1_53).to(device)\n",
    "fdata_54 = torch.load(f_f1_54).to(device)\n",
    "\n",
    "# Load loc data\n",
    "loc = torch.load(f_loc).to(device)\n",
    "\n",
    "# load the MRI\n",
    "MRI = torch.load(f_MRI).to(device)\n",
    "MRI_05 = torch.load(f_MRI_05).to(device)\n",
    "MRI_06 = torch.load(f_MRI_06).to(device)\n",
    "MRI_18 = torch.load(f_MRI_18).to(device)\n",
    "MRI_20 = torch.load(f_MRI_20).to(device)\n",
    "MRI_38 = torch.load(f_MRI_38).to(device)\n",
    "MRI_41 = torch.load(f_MRI_41).to(device)\n",
    "MRI_42 = torch.load(f_MRI_42).to(device)\n",
    "MRI_43 = torch.load(f_MRI_43).to(device)\n",
    "MRI_44 = torch.load(f_MRI_44).to(device)\n",
    "MRI_45 = torch.load(f_MRI_45).to(device)\n",
    "MRI_46 = torch.load(f_MRI_46).to(device)\n",
    "MRI_47 = torch.load(f_MRI_47).to(device)\n",
    "MRI_48 = torch.load(f_MRI_48).to(device)\n",
    "MRI_49 = torch.load(f_MRI_49).to(device)\n",
    "MRI_50 = torch.load(f_MRI_50).to(device)\n",
    "MRI_51 = torch.load(f_MRI_51).to(device)\n",
    "MRI_52 = torch.load(f_MRI_52).to(device)\n",
    "MRI_53 = torch.load(f_MRI_53).to(device)\n",
    "MRI_54 = torch.load(f_MRI_54).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5367c219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding parameters: \n",
    "tot_x_axis_len = 370+30 \n",
    "tot_y_axis_len = 444+30 \n",
    "\n",
    "# apply pad to location\n",
    "loc_new = utils.apply_pad_loc(loc, tot_x_axis_len, tot_y_axis_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb25beb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad E-field \n",
    "fdata_pad = utils.complex_padding(fdata, tot_y_axis_len, tot_x_axis_len)\n",
    "fdata_05_pad = utils.complex_padding(fdata_05, tot_y_axis_len, tot_x_axis_len)\n",
    "fdata_06_pad = utils.complex_padding(fdata_06, tot_y_axis_len, tot_x_axis_len)\n",
    "fdata_18_pad = utils.complex_padding(fdata_18, tot_y_axis_len, tot_x_axis_len)\n",
    "fdata_20_pad = utils.complex_padding(fdata_20, tot_y_axis_len, tot_x_axis_len)\n",
    "fdata_38_pad = utils.complex_padding(fdata_38, tot_y_axis_len, tot_x_axis_len)\n",
    "fdata_41_pad = utils.complex_padding(fdata_41, tot_y_axis_len, tot_x_axis_len)\n",
    "fdata_42_pad = utils.complex_padding(fdata_42, tot_y_axis_len, tot_x_axis_len)\n",
    "fdata_43_pad = utils.complex_padding(fdata_43, tot_y_axis_len, tot_x_axis_len)\n",
    "fdata_44_pad = utils.complex_padding(fdata_44, tot_y_axis_len, tot_x_axis_len)\n",
    "fdata_45_pad = utils.complex_padding(fdata_45, tot_y_axis_len, tot_x_axis_len)\n",
    "fdata_46_pad = utils.complex_padding(fdata_46, tot_y_axis_len, tot_x_axis_len)\n",
    "fdata_47_pad = utils.complex_padding(fdata_47, tot_y_axis_len, tot_x_axis_len)\n",
    "fdata_48_pad = utils.complex_padding(fdata_48, tot_y_axis_len, tot_x_axis_len)\n",
    "fdata_49_pad = utils.complex_padding(fdata_49, tot_y_axis_len, tot_x_axis_len)\n",
    "fdata_50_pad = utils.complex_padding(fdata_50, tot_y_axis_len, tot_x_axis_len)\n",
    "fdata_51_pad = utils.complex_padding(fdata_51, tot_y_axis_len, tot_x_axis_len)\n",
    "fdata_52_pad = utils.complex_padding(fdata_52, tot_y_axis_len, tot_x_axis_len)\n",
    "fdata_53_pad = utils.complex_padding(fdata_53, tot_y_axis_len, tot_x_axis_len)\n",
    "fdata_54_pad = utils.complex_padding(fdata_54, tot_y_axis_len, tot_x_axis_len)\n",
    "\n",
    "# pad MRI\n",
    "MRI_pad = utils.apply_pad(MRI, tot_x_axis_len, tot_y_axis_len)\n",
    "MRI_05_pad = utils.apply_pad(MRI_05, tot_x_axis_len, tot_y_axis_len)\n",
    "MRI_06_pad = utils.apply_pad(MRI_06, tot_x_axis_len, tot_y_axis_len)\n",
    "MRI_18_pad = utils.apply_pad(MRI_18, tot_x_axis_len, tot_y_axis_len)\n",
    "MRI_20_pad = utils.apply_pad(MRI_20, tot_x_axis_len, tot_y_axis_len)\n",
    "MRI_38_pad = utils.apply_pad(MRI_38, tot_x_axis_len, tot_y_axis_len)\n",
    "MRI_41_pad = utils.apply_pad(MRI_41, tot_x_axis_len, tot_y_axis_len)\n",
    "MRI_42_pad = utils.apply_pad(MRI_42, tot_x_axis_len, tot_y_axis_len)\n",
    "MRI_43_pad = utils.apply_pad(MRI_43, tot_x_axis_len, tot_y_axis_len)\n",
    "MRI_44_pad = utils.apply_pad(MRI_44, tot_x_axis_len, tot_y_axis_len)\n",
    "MRI_45_pad = utils.apply_pad(MRI_45, tot_x_axis_len, tot_y_axis_len)\n",
    "MRI_46_pad = utils.apply_pad(MRI_46, tot_x_axis_len, tot_y_axis_len)\n",
    "MRI_47_pad = utils.apply_pad(MRI_47, tot_x_axis_len, tot_y_axis_len)\n",
    "MRI_48_pad = utils.apply_pad(MRI_48, tot_x_axis_len, tot_y_axis_len)\n",
    "MRI_49_pad = utils.apply_pad(MRI_49, tot_x_axis_len, tot_y_axis_len)\n",
    "MRI_50_pad = utils.apply_pad(MRI_50, tot_x_axis_len, tot_y_axis_len)\n",
    "MRI_51_pad = utils.apply_pad(MRI_51, tot_x_axis_len, tot_y_axis_len)\n",
    "MRI_52_pad = utils.apply_pad(MRI_52, tot_x_axis_len, tot_y_axis_len)\n",
    "MRI_53_pad = utils.apply_pad(MRI_53, tot_x_axis_len, tot_y_axis_len)\n",
    "MRI_54_pad = utils.apply_pad(MRI_54, tot_x_axis_len, tot_y_axis_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645b83dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsampling data\n",
    "new_size = [int(a/n_subsample) for a in [tot_y_axis_len, tot_x_axis_len]] # FA: my code was here\n",
    "fdata = complex_resize(fdata_pad, new_size)\n",
    "fdata_05 = complex_resize(fdata_05_pad, new_size)\n",
    "fdata_06 = complex_resize(fdata_06_pad, new_size)\n",
    "fdata_18 = complex_resize(fdata_18_pad, new_size)\n",
    "fdata_20 = complex_resize(fdata_20_pad, new_size)\n",
    "fdata_38 = complex_resize(fdata_38_pad, new_size)\n",
    "fdata_41 = complex_resize(fdata_41_pad, new_size)\n",
    "fdata_42 = complex_resize(fdata_42_pad, new_size)\n",
    "fdata_43 = complex_resize(fdata_43_pad, new_size)\n",
    "fdata_44 = complex_resize(fdata_44_pad, new_size)\n",
    "fdata_45 = complex_resize(fdata_45_pad, new_size)\n",
    "fdata_46 = complex_resize(fdata_46_pad, new_size)\n",
    "fdata_47 = complex_resize(fdata_47_pad, new_size)\n",
    "fdata_48 = complex_resize(fdata_48_pad, new_size)\n",
    "fdata_49 = complex_resize(fdata_49_pad, new_size)\n",
    "fdata_50 = complex_resize(fdata_50_pad, new_size)\n",
    "fdata_51 = complex_resize(fdata_51_pad, new_size)\n",
    "fdata_52 = complex_resize(fdata_52_pad, new_size)\n",
    "fdata_53 = complex_resize(fdata_53_pad, new_size)\n",
    "fdata_54 = complex_resize(fdata_54_pad, new_size)\n",
    "\n",
    "# downsampling location\n",
    "loc = (loc_new/n_subsample).round().int()\n",
    "\n",
    "# downsampling MRI \n",
    "MRI    = torchvision.transforms.Resize(new_size, antialias=True)(MRI_pad.unsqueeze(0)).squeeze()\n",
    "MRI_05 = torchvision.transforms.Resize(new_size, antialias=True)(MRI_05_pad.unsqueeze(0)).squeeze()\n",
    "MRI_06 = torchvision.transforms.Resize(new_size, antialias=True)(MRI_06_pad.unsqueeze(0)).squeeze()\n",
    "MRI_18 = torchvision.transforms.Resize(new_size, antialias=True)(MRI_18_pad.unsqueeze(0)).squeeze()\n",
    "MRI_20 = torchvision.transforms.Resize(new_size, antialias=True)(MRI_20_pad.unsqueeze(0)).squeeze()\n",
    "MRI_38 = torchvision.transforms.Resize(new_size, antialias=True)(MRI_38_pad.unsqueeze(0)).squeeze()\n",
    "MRI_41 = torchvision.transforms.Resize(new_size, antialias=True)(MRI_41_pad.unsqueeze(0)).squeeze()\n",
    "MRI_42 = torchvision.transforms.Resize(new_size, antialias=True)(MRI_42_pad.unsqueeze(0)).squeeze()\n",
    "MRI_43 = torchvision.transforms.Resize(new_size, antialias=True)(MRI_43_pad.unsqueeze(0)).squeeze()\n",
    "MRI_44 = torchvision.transforms.Resize(new_size, antialias=True)(MRI_44_pad.unsqueeze(0)).squeeze()\n",
    "MRI_45 = torchvision.transforms.Resize(new_size, antialias=True)(MRI_45_pad.unsqueeze(0)).squeeze()\n",
    "MRI_46 = torchvision.transforms.Resize(new_size, antialias=True)(MRI_46_pad.unsqueeze(0)).squeeze()\n",
    "MRI_47 = torchvision.transforms.Resize(new_size, antialias=True)(MRI_47_pad.unsqueeze(0)).squeeze()\n",
    "MRI_48 = torchvision.transforms.Resize(new_size, antialias=True)(MRI_48_pad.unsqueeze(0)).squeeze()\n",
    "MRI_49 = torchvision.transforms.Resize(new_size, antialias=True)(MRI_49_pad.unsqueeze(0)).squeeze()\n",
    "MRI_50 = torchvision.transforms.Resize(new_size, antialias=True)(MRI_50_pad.unsqueeze(0)).squeeze()\n",
    "MRI_51 = torchvision.transforms.Resize(new_size, antialias=True)(MRI_51_pad.unsqueeze(0)).squeeze()\n",
    "MRI_52 = torchvision.transforms.Resize(new_size, antialias=True)(MRI_52_pad.unsqueeze(0)).squeeze()\n",
    "MRI_53 = torchvision.transforms.Resize(new_size, antialias=True)(MRI_53_pad.unsqueeze(0)).squeeze()\n",
    "MRI_54 = torchvision.transforms.Resize(new_size, antialias=True)(MRI_54_pad.unsqueeze(0)).squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515af32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_shape = fdata.shape\n",
    "print(data_shape)\n",
    "\n",
    "if model_type in ['LinearAE', 'LinearAE_M', 'ConvAE', 'ConvAE_M', 'unet',  'unet_M', 'UNet_M_LC' , 'wnet', 'wnet2']:\n",
    "    #Training data\n",
    "    x = torch.zeros(data_shape).to(device)\n",
    "    for i in range(loc.shape[0]):\n",
    "        impulse_input = torch.zeros(data_shape[1:])\n",
    "        r = loc[i, 1]\n",
    "        c = loc[i, 0]\n",
    "        impulse_input[r,c] = impulse_mag \n",
    "        x[i,:,:] = impulse_input\n",
    "elif model_type in ['Linear_M_Loc', 'LinearAE_M_Loc', 'ConvAE_M_Loc', 'UNet_M_Loc', 'UNet_M_Loc_strided', 'UNet_M_Loc_deepNet']:\n",
    "    #Training data\n",
    "    x = torch.zeros(data_shape).to(device)\n",
    "    for i in range(loc.shape[0]):\n",
    "        impulse_input = torch.zeros(data_shape[1:])\n",
    "        r = loc[i, 1]\n",
    "        c = loc[i, 0]\n",
    "        impulse_input[r,c] = MRI_loc_impulse_msg \n",
    "        x[i,:,:] = impulse_input\n",
    "elif model_type in ['Linear', 'Linear_M']: \n",
    "    x = loc.float().to(device)\n",
    "else:\n",
    "    raise Exception('Unknown model type')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f851bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final dataset concatenate: across patients\n",
    "x_train = torch.cat([x for i in range(14)], dim = 0)\n",
    "x_train_MRI = torch.cat([temp.unsqueeze(1).tile(x.shape[0], (1)).permute(1,0,2) \n",
    "                         for temp in [MRI, MRI_05, MRI_06, MRI_18, MRI_20, MRI_38, MRI_41, MRI_42, MRI_43, MRI_44, MRI_45, MRI_46, MRI_47, MRI_48]], dim = 0)\n",
    "fdata_train = torch.cat([fdata, fdata_05, fdata_06, fdata_18, fdata_20, fdata_38, fdata_41, fdata_42, fdata_43, fdata_44, fdata_45, fdata_46, fdata_47, fdata_48], dim = 0)\n",
    "\n",
    "x_val = torch.cat([x for i in range(6)], dim = 0)\n",
    "x_val_MRI = torch.cat([temp.unsqueeze(1).tile(x.shape[0], (1)).permute(1,0,2) \n",
    "                         for temp in [MRI_49, MRI_50, MRI_51, MRI_52, MRI_53, MRI_54]], dim = 0)\n",
    "fdata_val = torch.cat([fdata_49, fdata_50, fdata_51, fdata_52, fdata_53, fdata_54], dim = 0)\n",
    "print(x_val_MRI.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37520140",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdata_train_masked = fdata_train.cpu() * (x_train_MRI.cpu()>0)\n",
    "\n",
    "data_max_train = torch.max(fdata_train_masked.abs()).item()\n",
    "data_min_train = torch.min(fdata_train_masked.abs()).item()\n",
    "data_mean_train = torch.mean(fdata_train_masked).item()\n",
    "data_std_train = torch.std(fdata_train_masked.abs()).item()\n",
    "#print(f'max = {data_max_train},\\nmin = {data_min_train},\\nmean = {data_mean_train},\\nstd = {data_std_train}')\n",
    "\n",
    "fdata_val_masked = fdata_val.cpu() * (x_val_MRI.cpu()>0)\n",
    "data_max_val = torch.max(fdata_val_masked.abs()).item()\n",
    "data_min_val = torch.min(fdata_val_masked.abs()).item()\n",
    "data_mean_val = torch.mean(fdata_val_masked).item()\n",
    "data_std_val = torch.std(fdata_val_masked.abs()).item()\n",
    "#print(f'\\nmax = {data_max_val},\\nmin = {data_min_val},\\nmean = {data_mean_val},\\nstd = {data_std_val}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1449413",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FA_Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"My custom dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, loc, MRI, fdata, orig_size_down=orig_size_down, transformF=None, transformMRI=None):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            loc (string): location vector. Should be N x 2\n",
    "            MRI (string): MRI for each location. Should be N x W x H\n",
    "            fdata (callable): Impulse response. Should be N x W x H\n",
    "        \"\"\"\n",
    "        self.loc = loc\n",
    "        self.MRI = MRI\n",
    "        self.masks = (MRI > 0).float()\n",
    "        self.fdata = fdata\n",
    "        self.transformF = transformF\n",
    "        self.transformMRI = transformMRI\n",
    "        self.orig_size_down = orig_size_down\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.MRI.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return [self.loc[idx], self.transformMRI(self.MRI[idx]), self.masks[idx]], self.transformF(self.fdata[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d8bc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_shape = fdata.shape\n",
    "def transG(m, s):\n",
    "    return lambda x: (x - m)/s\n",
    "transF_train = transG(data_mean_train,data_std_train)\n",
    "transF_val   = transG(data_mean_val,data_std_val)\n",
    "transMRI = lambda x: (x.float()/255)*2-1\n",
    "\n",
    "# dataset\n",
    "trainset = FA_Dataset(x_train, x_train_MRI, fdata_train, transformF=transF_train, transformMRI=transMRI)\n",
    "testset  = FA_Dataset(x_val, x_val_MRI, fdata_val, transformF=transF_val, transformMRI=transMRI)\n",
    "\n",
    "# dataloader\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=n_batch, shuffle=True, num_workers=n_workers)# ask lorenxo\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=True, num_workers=n_workers)# ask lorenxo\n",
    "\n",
    "#Data properties\n",
    "d = torch.prod(torch.Tensor(list(data_shape[1:]))).int().item()# this is fine bcz outputs are still 434x362 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40005d57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Model hyperparameters that are the same for all models\n",
    "criterion = lambda x,y: ( (x-y).abs()**2 ).mean() #MSE\n",
    "\n",
    "momentum = 0.9\n",
    "weight_decay = 5e-4\n",
    "n_bottleneck = 64\n",
    "\n",
    "# defining new hyperparameter for random selection: (1e-7, 1e-2)\n",
    "rand_lr = 1+ 3*torch.rand(1) \n",
    "lr =  10**-rand_lr.item()\n",
    "print(lr)   \n",
    "\n",
    "# choose lernels in range (3, 12)\n",
    "rand_kernels = 3 + 9*torch.rand(1)\n",
    "n_kernel = int(rand_kernels)\n",
    "\n",
    "if n_kernel % 2 == 0:\n",
    "    n_kernel+=1\n",
    "print(n_kernel)   \n",
    "\n",
    "dropout_rand = 0.2*torch.rand(1).item()\n",
    "dropout_p = random.choice([0, dropout_rand])\n",
    "print(dropout_p)\n",
    "\n",
    "n_channels = random.choice([16, 32, 64])\n",
    "\n",
    "#Set model specific parameters\n",
    "if model_type == 'Linear':\n",
    "    n_epochs = 600\n",
    "    n_kernel = 0\n",
    "    n_showevery_epochs = 100\n",
    "    dropout_p = 0\n",
    "    \n",
    "    #Set the saving and plotting function\n",
    "    plot = utils.imshowNLinear \n",
    "    save = utils.imsaveNLinear \n",
    "    \n",
    "    #Make model\n",
    "    model_real = Linear(d, data_shape).to(device)\n",
    "    model_imag = Linear(d, data_shape).to(device)\n",
    "elif model_type == 'Linear_M':\n",
    "    n_epochs = 1500 \n",
    "    n_kernel = 0    \n",
    "    n_showevery_epochs = 100\n",
    "    dropout_p = 0\n",
    "    \n",
    "    #Set the saving and plotting function\n",
    "    plot = utils.imshowNLinear\n",
    "    save = utils.imsaveNLinear\n",
    "\n",
    "    #Make model\n",
    "    model_real = Linear_M(d, data_shape).to(device)\n",
    "    model_imag = Linear_M(d, data_shape).to(device)    \n",
    "elif model_type == 'Linear_M_Loc':\n",
    "    n_epochs = 1500 \n",
    "    n_kernel = 0    \n",
    "    n_showevery_epochs = 100\n",
    "    dropout_p = 0\n",
    "    \n",
    "    #Set the saving and plotting function\n",
    "    plot = utils.imshowNLinear\n",
    "    save = utils.imsaveNLinear\n",
    "\n",
    "    #Make model\n",
    "    model_real = Linear_M_Loc(d, data_shape).to(device)\n",
    "    model_imag = Linear_M_Loc(d, data_shape).to(device)        \n",
    "elif model_type == 'LinearAE':\n",
    "    n_epochs = 1000 \n",
    "    n_showevery_epochs = 100\n",
    "    n_kernel = 0\n",
    "    dropout_p = 0\n",
    "    \n",
    "    #Set the saving and plotting function\n",
    "    plot = utils.imshowN\n",
    "    save = utils.imsaveN\n",
    "\n",
    "    #Make model\n",
    "    model_real = LinearAE(d, n_bottleneck).to(device)\n",
    "    model_imag = LinearAE(d, n_bottleneck).to(device) \n",
    "elif model_type == 'LinearAE_M':\n",
    "    n_epochs = 6000 \n",
    "    n_showevery_epochs = 1000\n",
    "    n_kernel = 0\n",
    "    dropout_p = 0\n",
    "    \n",
    "    #Set the saving and plotting function\n",
    "    plot = utils.imshowN\n",
    "    save = utils.imsaveN\n",
    "\n",
    "    #Make model\n",
    "    model_real = LinearAE_M(d, n_bottleneck).to(device)\n",
    "    model_imag = LinearAE_M(d, n_bottleneck).to(device)  \n",
    "elif model_type == 'LinearAE_M_Loc':\n",
    "    n_epochs = 1500 \n",
    "    n_showevery_epochs = 100\n",
    "    n_kernel = 0\n",
    "    dropout_p = 0\n",
    "    \n",
    "    #Set the saving and plotting function\n",
    "    plot = utils.imshowN\n",
    "    save = utils.imsaveN\n",
    "\n",
    "    #Make model\n",
    "    model_real =  LinearAE_M_Loc(d, n_bottleneck).to(device)\n",
    "    model_imag = LinearAE_M_Loc(d, n_bottleneck).to(device)           \n",
    "elif model_type == 'ConvAE':\n",
    "    n_epochs = 5000\n",
    "    n_showevery_epochs = 100\n",
    "    \n",
    "    #Set the saving and plotting function\n",
    "    plot = utils.imshowN\n",
    "    save = utils.imsaveN\n",
    "    \n",
    "    #Make model\n",
    "    model_real = ConvAE().to(device)\n",
    "    model_imag = ConvAE().to(device)\n",
    "elif model_type == 'ConvAE_M':\n",
    "    n_epochs = 3000 \n",
    "    n_showevery_epochs = 100\n",
    "    \n",
    "    #Set the saving and plotting function\n",
    "    plot = utils.imshowN\n",
    "    save = utils.imsaveN\n",
    "    \n",
    "    #Make model\n",
    "    model_real = ConvAE_M_shallow(kernel_size=n_kernel, dropout_p=dropout_p).to(device)\n",
    "    model_imag = ConvAE_M_shallow(kernel_size=n_kernel, dropout_p=dropout_p).to(device)\n",
    "elif model_type == 'ConvAE_M_Loc':\n",
    "    n_epochs = 3000\n",
    "    n_showevery_epochs = 100\n",
    "    \n",
    "    #Set the saving and plotting function\n",
    "    plot = utils.imshowN\n",
    "    save = utils.imsaveN\n",
    "    \n",
    "    #Make model\n",
    "    model_real = ConvAE_M_Loc(kernel_size=n_kernel, dropout_p=dropout_p).to(device)\n",
    "    model_imag = ConvAE_M_Loc(kernel_size=n_kernel, dropout_p=dropout_p).to(device)\n",
    "    \n",
    "elif model_type == 'unet':\n",
    "    n_epochs = 3000\n",
    "    n_showevery_epochs = 100\n",
    "    n_stride = 1\n",
    "    \n",
    "    #Set the saving and plotting function\n",
    "    plot = utils.imshowN\n",
    "    save = utils.imsaveN\n",
    "    \n",
    "    #Make model\n",
    "    model_real = UNet(kernel_size=n_kernel, stride=n_stride, dropout_p=dropout_p).to(device)\n",
    "    model_imag = UNet(kernel_size=n_kernel, stride=n_stride, dropout_p=dropout_p).to(device)\n",
    "elif model_type == 'unet_M':\n",
    "    n_epochs = 3000\n",
    "    n_showevery_epochs = 100\n",
    "    n_stride = 1\n",
    "    \n",
    "    #Set the saving and plotting function\n",
    "    plot = utils.imshowN\n",
    "    save = utils.imsaveN\n",
    "    \n",
    "    #Make model\n",
    "    model_real = UNet_M(kernel_size=n_kernel, stride=n_stride, dropout_p=dropout_p).to(device)\n",
    "    model_imag = UNet_M(kernel_size=n_kernel, stride=n_stride, dropout_p=dropout_p).to(device)\n",
    "elif model_type == 'UNet_M_LC':\n",
    "    n_epochs = 3000\n",
    "    n_showevery_epochs = 100\n",
    "    n_stride = 1\n",
    "    \n",
    "    #Set the saving and plotting function\n",
    "    plot = utils.imshowN\n",
    "    save = utils.imsaveN\n",
    "    \n",
    "    #Make model\n",
    "    model_real = UNet_M_LC(kernel_size=n_kernel, stride=n_stride, dropout_p=dropout_p, n_channels_s1 = n_channels).to(device)\n",
    "    model_imag = UNet_M_LC(kernel_size=n_kernel, stride=n_stride, dropout_p=dropout_p, n_channels_s1 = n_channels).to(device)    \n",
    "elif model_type == 'UNet_M_Loc':\n",
    "    n_epochs = 4000\n",
    "    n_showevery_epochs = 500\n",
    "    n_stride = 1\n",
    "    \n",
    "    #Set the saving and plotting function\n",
    "    plot = utils.imshowN\n",
    "    save = utils.imsaveN\n",
    "    \n",
    "    #Make model\n",
    "    model_real = UNet_M_Loc(kernel_size=n_kernel, stride=n_stride, dropout_p=dropout_p, n_channels_s1 = n_channels).to(device)\n",
    "    model_imag = UNet_M_Loc(kernel_size=n_kernel, stride=n_stride, dropout_p=dropout_p, n_channels_s1 = n_channels).to(device)  \n",
    "elif model_type == 'UNet_M_Loc_strided':\n",
    "    n_epochs = 5000\n",
    "    n_showevery_epochs = 1 #500\n",
    "    n_stride = 2 \n",
    "    i_height = data_shape[1]\n",
    "    i_width = data_shape[2]    \n",
    "    \n",
    "    #Set the saving and plotting function\n",
    "    plot = utils.imshowN\n",
    "    save = utils.imsaveN\n",
    "    \n",
    "    #Make model\n",
    "    model_real = UNet_M_Loc_strided(kernel_size=n_kernel, stride=n_stride, dropout_p=dropout_p, n_channels_s1 = n_channels, height = i_height, width = i_width).to(device)\n",
    "    model_imag = UNet_M_Loc_strided(kernel_size=n_kernel, stride=n_stride, dropout_p=dropout_p, n_channels_s1 = n_channels, height = i_height, width = i_width).to(device)       \n",
    "elif model_type == 'UNet_M_Loc_deepNet':\n",
    "    if n_subsample > 3:\n",
    "        raise Exception('Image size not suitable for deepnet')\n",
    "\n",
    "    #lr = 1e-1\n",
    "    n_epochs = 4000\n",
    "    n_showevery_epochs = 500\n",
    "    n_stride = 1\n",
    "    \n",
    "    #Set the saving and plotting function\n",
    "    plot = utils.imshowN\n",
    "    save = utils.imsaveN\n",
    "    \n",
    "    #Make model\n",
    "    model_real = UNet_M_Loc_deepNet(kernel_size=n_kernel, stride=n_stride, dropout_p=dropout_p, n_channels_s1 = n_channels).to(device)\n",
    "    model_imag = UNet_M_Loc_deepNet(kernel_size=n_kernel, stride=n_stride, dropout_p=dropout_p, n_channels_s1 = n_channels).to(device)   \n",
    "elif model_type == 'wnet':\n",
    "    lr = 1e-4\n",
    "    n_epochs = 3000\n",
    "    n_showevery_epochs = 100\n",
    "    n_stride = 1\n",
    "    n_kernel = 5\n",
    "    \n",
    "    #Set the saving and plotting function\n",
    "    plot = utils.imshowN\n",
    "    save = utils.imsaveN\n",
    "    \n",
    "    #Make model\n",
    "    model_real = WNet(kernel_size=n_kernel, stride=n_stride).to(device)\n",
    "    model_imag = WNet(kernel_size=n_kernel, stride=n_stride).to(device)    \n",
    "elif model_type == 'wnet2':\n",
    "    lr = 1e-4\n",
    "    n_epochs = 3000\n",
    "    n_showevery_epochs = 100\n",
    "    n_stride = 1\n",
    "    n_kernel = 5\n",
    "    \n",
    "    #Set the saving and plotting function\n",
    "    plot = utils.imshowN\n",
    "    save = utils.imsaveN\n",
    "    \n",
    "    #Make model\n",
    "    model_real = WNet2(kernel_size=n_kernel, stride=n_stride).to(device)\n",
    "    model_imag = WNet2(kernel_size=n_kernel, stride=n_stride).to(device)   \n",
    "else:\n",
    "    raise Exception('Unknown model type')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f83c0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimizer\n",
    "optimizer = torch.optim.SGD(list(model_real.parameters()) + list(model_imag.parameters()), \n",
    "                            lr=lr, momentum=momentum, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae79906",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5868e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Train\n",
    "train_errors   = []\n",
    "test_accuracys = []\n",
    " \n",
    "\n",
    "lr_prev = lr\n",
    "for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    t_start = time()\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, MRIs, masks, labels = data[0][0].to(device), data[0][1].to(device), data[0][2].to(device),  data[1].to(device)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs_real = model_real(inputs, MRIs)\n",
    "        outputs_imag = model_imag(inputs, MRIs)\n",
    "        outputs = outputs_real + 1j*outputs_imag\n",
    "\n",
    "        #Mask the border\n",
    "        outputs = outputs * masks\n",
    "        labels = labels * masks\n",
    "\n",
    "        #Calc loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Save training loss\n",
    "        norm_factor_batch = len(trainloader.dataset)/n_batch\n",
    "        running_loss += loss.item()/norm_factor_batch\n",
    "    \n",
    "    #At the end of the epoch, save the train and test errors\n",
    "    train_error   = running_loss \n",
    "    test_accuracy = utils.get_test_accuracy(model_real, model_imag, dataset=testset, criterion=criterion)\n",
    "    train_errors.append(torch.Tensor([train_error]))\n",
    "    test_accuracys.append(test_accuracy.view(1,-1))\n",
    "    if (epoch + 1) % n_showevery_epochs == 0:\n",
    "        print(f'Epoch: {epoch+1}, Train error = {train_error:.3f}, Test accuracy = {test_accuracy.item():.3f}, Time elapsed = {time()-t_start:.1f}s')\n",
    "        plot([1-inputs[0], outputs[0], labels[0]], \n",
    "                 ['Input', 'Model Output', 'Ground Truth'],n_rows=1)\n",
    "        save([1-inputs[0], outputs[0], labels[0]], ['training_in', 'training_out', 'training_gt'], \n",
    "             tag='_epoch#' + str(epoch+1), path=os.path.join(f_output_images, f_subfolder, f_sub_sub_folder))\n",
    "        \n",
    "    #Step the scheduler\n",
    "    scheduler.step(train_error)\n",
    "    lr_now = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "    if lr_now != lr_prev:\n",
    "        print('Learning Rate =', lr_now, 'at epoch #' + str(epoch+1))\n",
    "        lr_prev = lr_now\n",
    "\n",
    "print('Finished Training')\n",
    "train_errors   = torch.concat(train_errors)\n",
    "test_accuracys = torch.concat(test_accuracys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb1fcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save models\n",
    "f_model_real = os.path.join(f_model_output, f_subfolder, model_type + '_real.pt')\n",
    "f_model_imag = os.path.join(f_model_output, f_subfolder, model_type + '_imag.pt')\n",
    "torch.save(model_real, f_model_real)\n",
    "torch.save(model_imag, f_model_imag)\n",
    "\n",
    "#Save errors\n",
    "f_error_train = os.path.join(f_model_output, f_subfolder, model_type + '_train_error.pt')\n",
    "f_error_test  = os.path.join(f_model_output, f_subfolder, model_type + '_test_error.pt')\n",
    "torch.save(train_errors, f_error_train)\n",
    "torch.save(test_accuracys, f_error_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98930ec1-5f87-4378-92aa-6917f07c79e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save best model\n",
    "def save_as_best_model():\n",
    "    #Save models\n",
    "    f_model_real = os.path.join(f_model_output, f_subfolder, model_type + '_real_best.pt')\n",
    "    f_model_imag = os.path.join(f_model_output, f_subfolder, model_type + '_imag_best.pt')\n",
    "    torch.save(model_real, f_model_real)\n",
    "    torch.save(model_imag, f_model_imag)\n",
    "    \n",
    "    #Save errors\n",
    "    f_error_train = os.path.join(f_model_output, f_subfolder, model_type + '_train_error_best.pt')\n",
    "    f_error_test  = os.path.join(f_model_output, f_subfolder, model_type + '_test_error_best.pt')\n",
    "    torch.save(train_errors, f_error_train)\n",
    "    torch.save(test_accuracys, f_error_test)\n",
    "\n",
    "if os.path.exists(f_log):\n",
    "    fid = open(f_log, 'r')\n",
    "    raw = fid.read()\n",
    "    fid.close()\n",
    "    \n",
    "    best_val = max([float(line.split('val_err: ')[1].split(',')[0]) for line in raw.split('\\n')[:-1]])\n",
    "    if test_accuracys[-1] < best_val:\n",
    "        #Save current model as best model\n",
    "        save_as_best_model()\n",
    "else:\n",
    "    #Save current model as best model\n",
    "    save_as_best_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f01511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Load models\n",
    "# f_model_real = os.path.join(f_model_output, f_subfolder, model_type + '_real.pt')\n",
    "# f_model_imag = os.path.join(f_model_output, f_subfolder, model_type + '_imag.pt')\n",
    "# model_real = torch.load(f_model_real, map_location=torch.device(device))\n",
    "# model_imag = torch.load(f_model_imag, map_location=torch.device(device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c94f917",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[15,5])\n",
    "\n",
    "x = np.arange(n_epochs)+1\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(x, train_errors)\n",
    "plt.title('Training Error')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE')\n",
    "ymin = train_errors.min()*0.9\n",
    "\n",
    "if model_type == 'Linear':\n",
    "    i_epoch = 25\n",
    "    plt.annotate(\"epoch \" + str(i_epoch), xy=(i_epoch, train_errors[i_epoch-1]), \n",
    "                 xytext=(i_epoch+20, train_errors[i_epoch-1]+1000), arrowprops=dict(arrowstyle=\"->\"))\n",
    "    i_epoch = 300\n",
    "    plt.annotate(\"epoch \" + str(i_epoch), xy=(i_epoch, train_errors[i_epoch-1]), \n",
    "                 xytext=(i_epoch-70, train_errors[i_epoch-1]+2000), arrowprops=dict(arrowstyle=\"->\"))\n",
    "    i_epoch = 500\n",
    "    plt.annotate(\"epoch \" + str(i_epoch), xy=(i_epoch, train_errors[i_epoch-1]), \n",
    "                 xytext=(i_epoch-100, train_errors[i_epoch-1]+1500), arrowprops=dict(arrowstyle=\"->\"))\n",
    "else:\n",
    "    plt.ylim([ymin,train_errors.quantile(.9)*1.1])\n",
    "\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(x, test_accuracys)\n",
    "plt.title('Validation Error')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE')\n",
    "ymin = test_accuracys.min()*0.9\n",
    "if model_type == 'Linear':\n",
    "    pass\n",
    "else:\n",
    "    plt.ylim([ymin,test_accuracys.quantile(.9)*1.1])\n",
    "plt.savefig(os.path.join(f_output_images, f_subfolder, f_sub_sub_folder, 'errors' + '.png'), pad_inches=0.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa88f77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Set to eval\n",
    "model_real.eval()\n",
    "model_imag.eval()\n",
    "\n",
    "real_PC_coeff = 0\n",
    "real_PC_coeff_val = 0\n",
    "loss = 0\n",
    "for i in range(len(testset)):\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        inputs, MRIs, masks, labels = testset[i][0][0].unsqueeze(0).to(device), testset[i][0][1].unsqueeze(0).to(device), testset[i][0][2].unsqueeze(0).to(device), testset[i][1].unsqueeze(0).to(device)\n",
    "        \n",
    "        # forward + backward + optimize\n",
    "        outputs_real = model_real(inputs, MRIs)\n",
    "        outputs_imag = model_imag(inputs, MRIs)\n",
    "        outputs = outputs_real + 1j*outputs_imag\n",
    "        \n",
    "        #Mask the border\n",
    "        outputs = outputs * masks\n",
    "        labels = labels * masks\n",
    "\n",
    "        # real-valued pearson correlation coefficient\n",
    "        real_PC_coeff_val = utils.pearson_corr_real(outputs, labels)\n",
    "        real_PC_coeff += real_PC_coeff_val / len(testset)\n",
    "\n",
    "        #Calculate loss\n",
    "        loss += criterion(outputs, labels) / len(testset)\n",
    "        \n",
    "        #Difference plot\n",
    "        with torch.no_grad():\n",
    "            diff = outputs - labels\n",
    "            diff_err = diff/labels*100\n",
    "                \n",
    "print(real_PC_coeff)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce9eda9-b9b5-4938-9902-5492f3f5055a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fid = open(f_log,'a')\n",
    "fid.write(f'lr: {lr}, seed: {seed}, kernel_size: {n_kernel}, model: {model_type}, val_err: {test_accuracys[-1].item()}, train_err: {train_errors[-1]}, n_epochs: {n_epochs}, dropout_p: {dropout_p}, n_channels: {n_channels}, MRI_loc_impulse_msg: {MRI_loc_impulse_msg}, real_PC_coeff: {real_PC_coeff}  \\n')\n",
    "fid.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa0953d-f694-45fc-98db-1a1fc6beaa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set to eval\n",
    "model_real.eval()\n",
    "model_imag.eval()\n",
    "\n",
    "# patient 1\n",
    "NumOfAntz = fdata_pad.shape[0]\n",
    "\n",
    "def perpatient_val_err(start, stop):\n",
    "    loss = 0\n",
    "    real_PC_coeff = 0\n",
    "    for i in range(start, stop):\n",
    "        with torch.no_grad():\n",
    "            inputs, MRIs, masks, labels = testset[i][0][0].unsqueeze(0).to(device), testset[i][0][1].unsqueeze(0).to(device), testset[i][0][2].unsqueeze(0).to(device), testset[i][1].unsqueeze(0).to(device)\n",
    "            # forward + backward + optimize\n",
    "            outputs_real = model_real(inputs, MRIs)\n",
    "            outputs_imag = model_imag(inputs, MRIs)\n",
    "            outputs = outputs_real + 1j*outputs_imag\n",
    "\n",
    "            #Mask the border\n",
    "            outputs = outputs * masks\n",
    "            labels = labels * masks\n",
    "\n",
    "            #Calculate loss\n",
    "            loss += criterion(outputs, labels) / NumOfAntz\n",
    "\n",
    "            # real-valued pearson correlation coefficient\n",
    "            real_PC_coeff_val = utils.pearson_corr_complex(outputs, labels)\n",
    "            real_PC_coeff += real_PC_coeff_val / NumOfAntz\n",
    "\n",
    "    return loss, real_PC_coeff\n",
    "\n",
    "val_err_P1, PC_P1 = perpatient_val_err(0, NumOfAntz)\n",
    "val_err_P2, PC_P2 = perpatient_val_err(NumOfAntz, NumOfAntz * 2 )\n",
    "val_err_P3, PC_P3 = perpatient_val_err(NumOfAntz * 2, NumOfAntz * 3 )\n",
    "val_err_P4, PC_P4 = perpatient_val_err(NumOfAntz * 3, NumOfAntz * 4 )\n",
    "val_err_P5, PC_P5 = perpatient_val_err(NumOfAntz * 4, NumOfAntz * 5 )\n",
    "val_err_P6, PC_P6 = perpatient_val_err(NumOfAntz * 5, NumOfAntz * 6 )\n",
    "\n",
    "val_err_all = [val_err_P1.item(), val_err_P2.item(), val_err_P3.item(), val_err_P4.item(), val_err_P5.item(), val_err_P6.item()]\n",
    "print(val_err_all)\n",
    "PC_all = [PC_P1.item(), PC_P2.item(), PC_P3.item(), PC_P4.item(), PC_P5.item(), PC_P6.item()]\n",
    "print(PC_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ba19d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# real part of complex PC: getting validation error per patient + per antenna\n",
    "#Set to eval\n",
    "model_real.eval()\n",
    "model_imag.eval()\n",
    "\n",
    "\n",
    "# NumOfAntz = fdata_pad.shape[0]\n",
    "def perPatient_perAnt_val_err(start, stop):\n",
    "    loss = []\n",
    "    real_PC_coeff = []\n",
    "    for i in range(start, stop):\n",
    "        with torch.no_grad():\n",
    "            inputs, MRIs, masks, labels = testset[i][0][0].unsqueeze(0).to(device), testset[i][0][1].unsqueeze(0).to(device), testset[i][0][2].unsqueeze(0).to(device), testset[i][1].unsqueeze(0).to(device)\n",
    "            # forward + backward + optimize\n",
    "            outputs_real = model_real(inputs, MRIs)\n",
    "            outputs_imag = model_imag(inputs, MRIs)\n",
    "            outputs = outputs_real + 1j*outputs_imag\n",
    "\n",
    "            #Mask the border\n",
    "            outputs = outputs * masks\n",
    "            labels = labels * masks\n",
    "\n",
    "            #Calculate loss\n",
    "            loss_rn = criterion(outputs, labels)\n",
    "            loss.append(torch.tensor([loss_rn]))\n",
    "\n",
    "\n",
    "            # real-valued pearson correlation coefficient\n",
    "            real_PC_coeff_val = utils.pearson_corr_complex(outputs, labels)\n",
    "            real_PC_coeff.append(torch.tensor([real_PC_coeff_val]))\n",
    "\n",
    "    return loss, real_PC_coeff\n",
    "\n",
    "val_err_all, PC_all = perPatient_perAnt_val_err(0, NumOfAntz * 6)\n",
    "\n",
    "val_err_all   = torch.concat(val_err_all)\n",
    "PC_all = torch.concat(PC_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60418a3d-4724-4cd7-85fb-bcbafa229bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting different colorbars for OP, GT, and DIFF\n",
    "plt.set_cmap('viridis')\n",
    "\n",
    "#Set to eval\n",
    "model_real.eval()\n",
    "model_imag.eval()\n",
    "\n",
    "\n",
    "#for i in range(len(testset)):\n",
    "i = 1 - 1\n",
    "print(i)\n",
    "with torch.no_grad():\n",
    "    inputs, MRIs, masks, labels = testset[i][0][0].unsqueeze(0).to(device), testset[i][0][1].unsqueeze(0).to(device), testset[i][0][2].unsqueeze(0).to(device), testset[i][1].unsqueeze(0).to(device)\n",
    "\n",
    "    # forward + backward + optimize\n",
    "    outputs_real = model_real(inputs, MRIs)\n",
    "    outputs_imag = model_imag(inputs, MRIs)\n",
    "    outputs = outputs_real + 1j*outputs_imag\n",
    "\n",
    "    #Mask the border\n",
    "    outputs = outputs * masks\n",
    "    labels = labels * masks\n",
    "\n",
    "    #Difference plot\n",
    "    diff = outputs - labels\n",
    "    rel_err = (outputs.abs() - labels.abs())/(labels.abs()[labels.abs().isnan()==False].max()) * 100\n",
    "    rel_err_ang = (outputs.angle().abs() - labels.angle().abs())/(labels.angle().abs()[labels.angle().abs().isnan()==False].max()) * 100\n",
    "\n",
    "    masks2 = masks.clone()\n",
    "    masks2[masks2==0] = torch.nan\n",
    "    labels = labels * masks2\n",
    "    outputs = outputs * masks2\n",
    "    diff = diff * masks2\n",
    "    rel_err = rel_err * masks2\n",
    "    rel_err_ang = rel_err_ang * masks2\n",
    "\n",
    "    LIST = [labels[0], outputs[0], diff[0], rel_err[0]]\n",
    "    TITLES = ['Ground Truth', 'Model Output','DIFF', 'Rel_err']\n",
    "\n",
    "    plot(LIST, TITLES, n_rows=1, colorbar=True, log=True)\n",
    "\n",
    "    #save(LIST, TITLES, tag='_L' + str(i+1) + '_400MHz', path=os.path.join(f_output_images, f_subfolder, f_sub_sub_folder), figsize_width=17, figsize_height=6, log=True, clim=None)\n",
    "\n",
    "#    break\n",
    "LIST = [labels[0], outputs[0], diff[0], rel_err[0]]\n",
    "n_cols = 4\n",
    "\n",
    "max_plots = torch.zeros(n_cols)\n",
    "min_plots = torch.zeros(n_cols)\n",
    "\n",
    "for ind in range(len(LIST)):\n",
    "    max_plots[ind] = LIST[ind].abs()[LIST[ind].abs().isnan() == False].max()\n",
    "    min_plots[ind] = LIST[ind].abs()[LIST[ind].abs().isnan() == False].min()\n",
    "\n",
    "max_plt = torch.log(max_plots[1])\n",
    "min_plt = torch.log(min_plots[1])\n",
    "\n",
    "# plt.jet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f3eced-2726-4b08-91aa-b37eff45a82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PAPERs+relative error: plotting same colorbars for GT, OP, and different for Diff \n",
    "LIST = [labels[0], outputs[0], diff[0], rel_err[0]]\n",
    "n_cols = len(LIST)\n",
    "log = True\n",
    "\n",
    "max_plots = torch.zeros(n_cols)\n",
    "min_plots = torch.zeros(n_cols)\n",
    "\n",
    "for ind in range(len(LIST)):\n",
    "    max_plots[ind] = LIST[ind].abs()[LIST[ind].abs().isnan() == False].max()\n",
    "    min_plots[ind] = LIST[ind].abs()[LIST[ind].abs().isnan() == False].min()\n",
    "\n",
    "# print(max_plots)\n",
    "# print(min_plots)\n",
    "\n",
    "if log == True:\n",
    "    max_a = max_plots[0].max().log()\n",
    "    min_a = min_plots[0].min().log()\n",
    "else:\n",
    "    max_a = max_plots[0].max()\n",
    "    min_a = min_plots[0].min()\n",
    "\n",
    "max_b = max_plots[0]\n",
    "min_b = min_plots[0]\n",
    "\n",
    "max_c = max_plots[-1]\n",
    "min_c = min_plots[-1]\n",
    "\n",
    "# print(max_b)\n",
    "# print(min_b)\n",
    "\n",
    "f, ax = plt.subplots(1, 4, figsize=(14, 3.5))\n",
    "if log == True:\n",
    "    l0 = ax[0].imshow(LIST[0].cpu().abs().log(), vmin=min_a, vmax=max_a)\n",
    "    ax[0].invert_yaxis()\n",
    "    ax[0].axis('off')\n",
    "    l1 = ax[1].imshow(LIST[1].cpu().abs().log(), vmin=min_a, vmax=max_a)\n",
    "    ax[1].invert_yaxis()\n",
    "    ax[1].axis('off')\n",
    "    l2 = ax[2].imshow(LIST[2].cpu().abs(), vmin=min_b, vmax=max_b)\n",
    "    ax[2].invert_yaxis()\n",
    "    ax[2].axis('off')\n",
    "    l3 = ax[3].imshow(LIST[3].cpu().abs(), vmin=min_c, vmax=max_c)\n",
    "    ax[3].invert_yaxis()\n",
    "    ax[3].axis('off')\n",
    "else:\n",
    "    l0 = ax[0].imshow(LIST[0].cpu().abs(), vmin=min_a, vmax=max_a)\n",
    "    ax[0].invert_yaxis()\n",
    "    ax[0].axis('off')\n",
    "    l1 = ax[1].imshow(LIST[1].cpu().abs(), vmin=min_a, vmax=max_a)\n",
    "    ax[1].invert_yaxis()\n",
    "    ax[1].axis('off')\n",
    "    l2 = ax[2].imshow(LIST[2].cpu().abs(), vmin=min_b, vmax=max_b)\n",
    "    ax[2].invert_yaxis()\n",
    "    ax[2].axis('off')\n",
    "    # l3 = ax[3].imshow(LIST[3].cpu().abs(), vmin=min_b, vmax=max_b)\n",
    "    # ax[3].invert_yaxis()\n",
    "    # ax[3].axis('off')\n",
    "\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "f.colorbar(l0, ax=list(ax[0:2]), orientation='vertical', pad=0.03, shrink=0.85)\n",
    "f.colorbar(l2, ax=ax[2], orientation='vertical', pad=0.03, shrink=0.85)\n",
    "f.colorbar(l3, ax=ax[3], orientation='vertical', pad=0.03, shrink=0.85)\n",
    "plt.set_cmap('viridis')\n",
    "\n",
    "#plt.jet()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "path=os.path.join(f_output_images, f_subfolder, f_sub_sub_folder)\n",
    "f.savefig(os.path.join(path, freq + '_' + str(i+1) +'_' + model_type + '.png'), dpi=150, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f153eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "###new with 2 colorbars\n",
    "#new: (0, 2pi) angle range: plotting ANGLES: same colorbars for GT, OP, and  Diff \n",
    "plt.set_cmap('twilight_shifted')\n",
    "\n",
    "LIST = [labels[0], outputs[0], diff[0], rel_err_ang[0]]\n",
    "n_cols = len(LIST)\n",
    "log = False # log will always be set to false for angles\n",
    "\n",
    "max_a = 2*torch.pi\n",
    "min_a = 0\n",
    "\n",
    "max_b = LIST[3].abs()[LIST[3].abs().isnan() == False].max()\n",
    "min_b = LIST[3].abs()[LIST[3].abs().isnan() == False].min()\n",
    "\n",
    "f, ax = plt.subplots(1, 4, figsize=(14, 3.5))\n",
    "# plt.jet()\n",
    "\n",
    "pi_def = torch.tensor(torch.pi);\n",
    "\n",
    "l0 = ax[0].imshow(torch.remainder(LIST[0].cpu().angle(),2*pi_def), vmin=min_a, vmax=max_a, interpolation=\"nearest\")\n",
    "ax[0].invert_yaxis()\n",
    "ax[0].axis('off')\n",
    "l1 = ax[1].imshow(torch.remainder(LIST[1].cpu().angle(),2*pi_def), vmin=min_a, vmax=max_a, interpolation=\"nearest\")\n",
    "ax[1].invert_yaxis()\n",
    "ax[1].axis('off')\n",
    "metric_b = (torch.remainder(LIST[0].cpu().angle(),2*pi_def) - torch.remainder(LIST[1].cpu().angle(),2*pi_def)).abs()\n",
    "angle_dist = torch.minimum(metric_b, 2*torch.pi - metric_b)\n",
    "l2 = ax[2].imshow(angle_dist, vmin=min_a, vmax=max_a)\n",
    "ax[2].invert_yaxis()\n",
    "ax[2].axis('off')\n",
    "#l3 = ax[3].imshow(LIST[3].cpu(), vmin=min_b, vmax=max_b)\n",
    "ax[3].invert_yaxis()\n",
    "ax[3].axis('off')\n",
    "\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "f.colorbar(l0, ax=list(ax[0:2]), orientation='vertical', pad=0.03, shrink=0.85)\n",
    "f.colorbar(l2, ax=ax[2], orientation='vertical', pad=0.03, shrink=0.85)\n",
    "# f.colorbar(l3, ax=ax[3], orientation='vertical', pad=0.03, shrink=0.6)\n",
    "plt.set_cmap('twilight_shifted')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "path=os.path.join(f_output_images, f_subfolder, f_sub_sub_folder)\n",
    "f.savefig(os.path.join(path, 'angles'+ freq + '_' + str(i+1) +'_' +  model_type + '.png'), dpi=150, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f12b83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554a7302",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2cba8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
